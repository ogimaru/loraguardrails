@PREAMBLE{
	"\providecommand{\noopsort}[1]{}" 
	# "\providecommand{\singleletter}[1]{#1}%" 
}

@article{bhargava2023s,
	title   = {What's the Magic Word? A Control Theory of LLM Prompting},
	author  = {Bhargava, Aman and Witkowski, Cameron and Looi, Shi-Zhuo and Thomson, Matt},
	journal = {arXiv preprint arXiv:2310.04444},
	year    = {2023}
}

@article{agarwal2024many,
	title   = {Many-shot in-context learning},
	author  = {Agarwal, Rishabh and Singh, Avi and Zhang, Lei M and Bohnet, Bernd and Chan, Stephanie and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and Co-Reyes, John D and Chu, Eric and others},
	journal = {arXiv preprint arXiv:2404.11018},
	year    = {2024}
}

@article{zhao2024probabilistic,
	title   = {Probabilistic inference in language models via twisted sequential monte carlo},
	author  = {Zhao, Stephen and Brekelmans, Rob and Makhzani, Alireza and Grosse, Roger},
	journal = {arXiv preprint arXiv:2404.17546},
	year    = {2024}
}

@article{lew2023sequential,
	title   = {Sequential monte carlo steering of large language models using probabilistic programs},
	author  = {Lew, Alexander K and Zhi-Xuan, Tan and Grand, Gabriel and Mansinghka, Vikash K},
	journal = {arXiv preprint arXiv:2306.03081},
	year    = {2023}
}

@inproceedings{loulasyntactic,
	title={Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo},
	author={Loula, Jo{\~a}o and LeBrun, Benjamin and Du, Li and Lipkin, Ben and Pasti, Clemente and Grand, Gabriel and Liu, Tianyu and Emara, Yahya and Freedman, Marjorie and Eisner, Jason and others},
	booktitle={The Thirteenth International Conference on Learning Representations},
	year={2024}
}

@article{inan2023llama,
	title   = {Llama guard: Llm-based input-output safeguard for human-ai conversations},
	author  = {Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
	journal = {arXiv preprint arXiv:2312.06674},
	year    = {2023}
}

@article{hu2021lora,
	title   = {LoRA: Low-Rank Adaptation of Large Language Models},
	author  = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Chen, Weizhu},
	journal = {arXiv preprint arXiv:2106.09685},
	year    = {2021}
}

@article{shao2023character,
	title   = {Character-llm: A trainable agent for role-playing},
	author  = {Shao, Yunfan and Li, Linyang and Dai, Junqi and Qiu, Xipeng},
	journal = {arXiv preprint arXiv:2310.10158},
	year    = {2023}
}
@inproceedings{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, A and Shazeer, N and Parmar, N and Uszkoreit, J and Jones, L and Gomez, A and Kaiser, L and Polosukhin, I},
	booktitle={NIPS},
	year={2017}
}

@article{sumers2023cognitive,
	title   = {Cognitive architectures for language agents},
	author  = {Sumers, Theodore R and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L},
	journal = {arXiv preprint arXiv:2309.02427},
	year    = {2023}
}

@article{hong2024orpo,
	title   = {Orpo: Monolithic preference optimization without reference model},
	author  = {Hong, Jiwoo and Lee, Noah and Thorne, James},
	journal = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
	pages   = {11170--11189},
	year    = {2024}
}

@misc{llama_format,
	author = {{Meta AI}},
	title  = {Model Cards and Prompt Formats - {LLaMA} 3.1},
	year   = {2024},
	url    = {https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/},
	note   = {Accessed: 2024-04-07}
}

@article{zeng2024shieldgemma,
	title={Shieldgemma: Generative ai content moderation based on gemma},
	author={Zeng, Wenjun and Liu, Yuchi and Mullins, Ryan and Peran, Ludovic and Fernandez, Joe and Harkous, Hamza and Narasimhan, Karthik and Proud, Drew and Kumar, Piyush and Radharapu, Bhaktipriya and others},
	journal={arXiv preprint arXiv:2407.21772},
	year={2024}
}

@article{han2024wildguard,
	title={Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms},
	author={Han, Seungju and Rao, Kavel and Ettinger, Allyson and Jiang, Liwei and Lin, Bill Yuchen and Lambert, Nathan and Choi, Yejin and Dziri, Nouha},
	journal={arXiv preprint arXiv:2406.18495},
	year={2024}
}

@article{hendrycks2023overview,
	title={An overview of catastrophic ai risks},
	author={Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas},
	journal={arXiv preprint arXiv:2306.12001},
	year={2023}
}

@article{ghosh2024aegis,
	title={AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts},
	author={Ghosh, Shaona and Varshney, Prasoon and Galinkin, Erick and Parisien, Christopher},
	journal={arXiv preprint arXiv:2404.05993},
	year={2024}
}

@inproceedings{leerlaif,
	title={RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback},
	author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie Ren and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and others},
	booktitle={Forty-first International Conference on Machine Learning}
}


@article{cao2024worst,
	title={On the Worst Prompt Performance of Large Language Models},
	author={Cao, Bowen and Cai, Deng and Zhang, Zhisong and Zou, Yuexian and Lam, Wai},
	journal={arXiv preprint arXiv:2406.10248},
	year={2024}
}

@article{schulhoff2024prompt,
	title={The Prompt Report: A Systematic Survey of Prompting Techniques},
	author={Schulhoff, Sander and Ilie, Michael and Balepur, Nishant and Kahadze, Konstantine and Liu, Amanda and Si, Chenglei and Li, Yinheng and Gupta, Aayush and Han, HyoJung and Schulhoff, Sevien and others},
	journal={arXiv preprint arXiv:2406.06608},
	year={2024}
}

@article{ishikawacapturing,
	title={Capturing Individuals' Communication Styles Using Large Language Models},
	author={Ishikawa, Yuya and Tsubaki, Kyosuke and Kitagawa, Kazuki and Kanno, Kazuya and Iwahashi, Nanami and Regan, Ciaran and Oka, Mizuki},
	journal={arXiv preprint arXiv:2403.12568},
	year={2024}
}

@article{zhang2023instruction,
	title={Instruction tuning for large language models: A survey},
	author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
	journal={arXiv preprint arXiv:2308.10792},
	year={2023}
}


@article{ouyang2022training,
	title={Training language models to follow instructions with human feedback},
	author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
	journal={Advances in neural information processing systems},
	volume={35},
	pages={27730--27744},
	year={2022}
}

@article{code,
	title = {Behavior {LoRA} Guardrail},
	author = {Gaudiy AI Research},
	year = {2024},
	publisher = {GitHub},
	journal = {GitHub repository},
	url = {https://github.com/gaudiy/behavior_LoRA_guardrail}
}

@article{wang2023orthogonal,
  title={Orthogonal subspace learning for language model continual learning},
  author={Wang, Xiao and Chen, Tianze and Ge, Qiming and Xia, Han and Bao, Rong and Zheng, Rui and Zhang, Qi and Gui, Tao and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2310.14152},
  year={2023}
}

@inproceedings{hu2024learn,
  title={Learn to Preserve and Diversify: Parameter-Efficient Group with Orthogonal Regularization for Domain Generalization},
  author={Hu, Jiajun and Zhang, Jian and Qi, Lei and Shi, Yinghuan and Gao, Yang},
  booktitle={European Conference on Computer Vision},
  pages={198--216},
  year={2024},
  organization={Springer}
}

@software{unsloth,
  title = {Unsloth},
  author = {Daniel Han, Michael Han and Unsloth team},
  url = {http://github.com/unslothai/unsloth},
  year = {2023}
}